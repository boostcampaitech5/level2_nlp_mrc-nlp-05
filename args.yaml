wandb:
    use: True
    project: mrc_project
    name: 'korquad_finetuning_2' # 뒤에 model_name | batch_size | max_epoch | lr | warmup_steps | weight_decay가 자동으로 붙습니다. 비워두면 이것만 나옵니다
    
model:
    model_name: klue/roberta-large
    saved_model_path: /opt/ml/models/finetuning_dataset_roberta # inference 때 사용 # 1차 fine-tuning시 사용
    config_name: null
    tokenizer_name: null
    retrieval_tokenizer: monologg/koelectra-base-v3-finetuned-korquad # sparse_embedding.bin 이나 bm25_sparse_embedding.bin 은 tokenizer에 따라 달라집니다. 바꿀 경우, 삭제 후 inference하셔야 합니다.

data:
    train_dataset_name: /opt/ml/input/data/train_dataset
    test_dataset_name: /opt/ml/input/data/test_dataset # eval inference에서 train
    overwrite_cache: False
    preprocessing_num_workers: null
    max_seq_length: 384
    pad_to_max_length: False
    doc_stride: 128
    max_answer_length: 30
    eval_retrieval: True
    num_clusters: 64
    top_k_retrieval: 30
    use_faiss: False
    retrieval_type : bm25 # tfidf, bm25
    data_type: original # original, korquad, korquad_hard, mix, mix_hard (설명은 prepare_dataset.py에)

train:
    batch_size: 16
    max_epoch: 5
    learning_rate: 8.0e-6
    eval_step: 100
    logging_step: 100
    save_step: 100
    gradient_accumulation : 1
    do_train: True
    do_eval: False # train에서 True, 제출용 inference에서는 False, eval inference는 True
    do_predict : True # train에서 False, 제출용 inference에서는 True, eval inference에서는 False
    train_output_dir: /opt/ml/models/train_dataset
    inference_output_dir: /opt/ml/outputs/test_dataset
    warmup_steps: 0
    weight_decay: 0.0
    seed: 42